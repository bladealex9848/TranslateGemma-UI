# An치lisis de Rendimiento: Servidor Ollama Personalizado

Este documento detalla los resultados de las pruebas de rendimiento realizadas contra un **servidor Ollama personalizado** para determinar la configuraci칩n 칩ptima de TranslateGemma-UI.

**Fecha**: 18 de Enero, 2026
**Ubicaci칩n de Prueba**: Remota

---

## 1. Modelos Disponibles

El servidor expone los siguientes modelos relevantes:
*   `translategemma:12b`: Modelo equilibrado (Google DeepMind).
*   `translategemma:latest`: Etiqueta puntero (Parece corresponder a la variante 4B u optimizada).
*   `gemma3:1b`: Modelo general ultraligero.

---

## 2. Resultados del Benchmark

Pruebas realizadas con traducci칩n Ingl칠s -> Espa침ol (Prompt est치ndar).

| Modelo | Tiempo Promedio (s) | Velocidad (Tokens/s) | Calidad Percibida |
| :--- | :--- | :--- | :--- |
| **`gemma3:1b`** | 1.27s | **26.84 t/s** | Buena (Prompt simple) |
| **`translategemma:latest`** | 1.73s | **17.00 t/s** | Excelente |
| **`translategemma:12b`** | 3.80s | 5.93 t/s | Excelente |

### Gr치fico de Velocidad (Tok/s)

```mermaid
gantt
    title Velocidad de Generaci칩n (M치s largo es mejor)
    dateFormat X
    axisFormat %s
    
    section Modelos
    Gemma3 1B (26.8 t/s)       : 0, 26
    TranslateGemma Latest(17 t/s) : 0, 17
    TranslateGemma 12B (5.9 t/s)  : 0, 6
```

---

## 3. An치lisis e Interpretaci칩n

### `translategemma:latest` (Ganador para Default) 游끥
Con **17 tokens por segundo**, este modelo ofrece una experiencia de usuario "casi instant치nea". Es 3 veces m치s r치pido que la versi칩n 12B. Para una interfaz web interactiva, la latencia es el factor m치s cr칤tico.
*   **Recomendaci칩n**: Usar como `DEFAULT_MODEL`.

### `translategemma:12b`
Con **~6 tokens por segundo**, es notablemente m치s lento. Aunque la generaci칩n es fluida (m치s r치pido de lo que se lee), se siente "pesado" para frases cortas.
*   **Recomendaci칩n**: Ofrecer como opci칩n "Modo Alta Precisi칩n" en la configuraci칩n de la UI.

### `gemma3:1b`
Sorprendentemente r치pido y capaz, pero no est치 afinado espec칤ficamente para traducci칩n (instrucciones complejas podr칤an fallar).
*   **Recomendaci칩n**: Mantener como backup para dispositivos m칩viles extremos.

---

## 4. Configuraci칩n Final Aplicada

Se ha actualizado el archivo `.env` del proyecto con estos valores 칩ptimos:

```env
OLLAMA_HOST=https://ollama.alexanderoviedofadul.dev/
OLLAMA_MODEL=translategemma:latest
OLLAMA_TIMEOUT=60  # Reducido de 120s debido a la alta velocidad detectada
```

---

## 5. Conclusiones

El servidor personalizado est치 **perfectamente capacitado** para soportar el tr치fico de TranslateGemma-UI. La latencia de red es m칤nima y el tiempo de inferencia es competitivo con APIs comerciales.
